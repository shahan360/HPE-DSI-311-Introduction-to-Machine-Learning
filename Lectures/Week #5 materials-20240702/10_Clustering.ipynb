{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=5, cluster_std=.8, random_state=seed)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 : K Means\n",
    "\n",
    "numberOfGroups = 5\n",
    "kmeans = KMeans(n_clusters=numberOfGroups, random_state=seed)\n",
    "kmeans.fit(X)\n",
    "groups = kmeans.labels_\n",
    "\n",
    "print(kmeans.cluster_centers_)\n",
    "print(groups)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c=groups, cmap='rainbow')   # color each group differently\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], color='black', marker='x')  # plot the centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning\n",
    "\n",
    "There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-learn employs several tricks to mitigate this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method to validate the number of clusters is the elbow method. The idea of the elbow method is to run k-means clustering on the dataset for a range of values of k (say, k from 1 to 10 in the examples above), and for each value of k calculate the sum of squared errors (SSE).\n",
    "\n",
    "Then, plot a line chart of the SSE for each value of k. If the line chart looks like an arm, then the \"elbow\" on the arm is the value of k that is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++')\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "with plt.style.context(('fivethirtyeight')):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(1, 10), wcss)\n",
    "    plt.title('The Elbow Method with k-means++\\n',fontsize=25)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.ylabel('WCSS (within-cluster sums of squares)')\n",
    "    plt.vlines(x=3,ymin=0,ymax=100,linestyles='--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 : Agglomerative Clustering\n",
    "\n",
    "kmeans = AgglomerativeClustering(n_clusters=numberOfGroups)\n",
    "kmeans.fit(X)\n",
    "groups = kmeans.labels_\n",
    "print(groups)\n",
    "plt.scatter(X[:,0],X[:,1], c=groups, cmap='rainbow')   # color each group differently"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
