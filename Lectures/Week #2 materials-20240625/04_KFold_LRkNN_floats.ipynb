{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# First, letâ€™s import all of the modules, functions and objects we are going to use in this tutorial.\n",
    "\n",
    "# Pandas for data handling\n",
    "import pandas # https://pandas.pydata.org/\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# pretty tables\n",
    "from IPython.display import display\n",
    "\n",
    "# NumPy for numerical computing\n",
    "import numpy # https://numpy.org/\n",
    "\n",
    "# MatPlotLib + Seaborn for visualization\n",
    "import matplotlib.pyplot as pl  # https://matplotlib.org/\n",
    "import seaborn as sns   # https://seaborn.pydata.org/\n",
    "\n",
    "# assessment\n",
    "from sklearn import model_selection # for model comparisons\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data from file ...')  # Now let's load the data\n",
    "dataset = pandas.read_csv('floats.csv') # default is header=infer, change if column names are not in first row\n",
    "print('done \\n')\n",
    "\n",
    "print('Removing rows with missing data ...')  # Make things simple\n",
    "dataset = dataset.dropna()  # default is to drop any row that contains at least one missing value\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up a problem: Can we predict 'callSign' using these three features: 'Depth', 'Temperature', 'Salinity' ?\n",
    "\n",
    "print('Reading list of problem variables X and Y...')\n",
    "X_name = [ 'Depth', 'Temperature', 'Salinity' ] # columns to focus on as predictors\n",
    "X = dataset[X_name]   # only keep these columns as features\n",
    "y_name = 'callSign'     # column to focus on as target\n",
    "y = dataset[y_name]   # only keep this column as label \n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out the test dataset for summative evaluation\n",
    "\n",
    "# We reset the random number seed before each run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable.\n",
    "\n",
    "# Train, test split\n",
    "print('Partitioning data into two parts: train (for formative development) and test (for summative evaluation) ...')\n",
    "test_size = 0.20   # means 20 percent\n",
    "seed = 42 # setting the seed allows for repeatability\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose the formative scoring method\n",
    "\n",
    "print('Reading list of scoring methods to use during model development ...')\n",
    "scoring = 'accuracy'\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose the Algorithms\n",
    "\n",
    "print('Reading list of algorithms to train ...')\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(multi_class='multinomial', class_weight=None, solver='lbfgs', max_iter=250)))\n",
    "models.append(('Ridge', RidgeClassifier(alpha=10)))\n",
    "models.append(('kNN', KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', p=2)))\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will use 5-fold cross validation to estimate the models' capacity to generalize on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it is time to develop (train and validate) the models on the formative data set\n",
    "\n",
    "k4folds = 5   # This will split our formative dataset into five parts;\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:   # Select each model in turn\n",
    "    print(\" ++ NOW WORKING ON ALGORITHM %s ++\" % name)\n",
    "    print(\"Splitting data into %s folds\" % k4folds)\n",
    "    kfold = model_selection.KFold(n_splits=k4folds, random_state=seed, shuffle=True)   # fit the model using four parts at a time and then validate it on the oher part that was set aside; and repeat five times.\n",
    "    print(\"Training model and validating it on each fold\")\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"algorithm %s %s results: mean = %f (std = %f)\" % (name, scoring, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "print('done \\n')   # Note that we have not used any of the test data yet!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Select Most Promising Model\n",
    "We now have 3 models and accuracy estimations for each. We need to compare the models to each other and select the most promising one.\n",
    "\n",
    "In this case, we can see that it looks like KNN has the largest estimated accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical comparison \n",
    "# We can also create a plot of the model evaluation results \n",
    "# and compare the spread and the mean accuracy of each model. \n",
    "# There is a population of accuracy measures for each algorithm \n",
    "# because each algorithm was evaluated several times (k fold cross validation).\n",
    "fig = pl.figure()\n",
    "fig.suptitle('Algorithm Comparison based on %s' % scoring)\n",
    "ax = fig.add_subplot(111)\n",
    "pl.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Make Predictions\n",
    "The KNN algorithm is very simple and was the most capable model based on our validation. \n",
    "\n",
    "Now we want to see how this estimate holds up on our separate test set.\n",
    "\n",
    "This will give us an independent final verdict on the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we retrain the most capable model using ALL of the training data\n",
    "selected_model = KNeighborsClassifier()\n",
    "selected_model.fit(X_train, y_train)\n",
    "\n",
    "# Next we use the trained KNN model on the test set and summarize the results \n",
    "# as a final accuracy score, a confusion matrix and a classification report.\n",
    "predictions = selected_model.predict(X_test)\n",
    "print(\" ++++ Detailed classification report for the selected model ++++ \" )\n",
    "print(\"Algorithm %s \" % selected_model)\n",
    "print(\"This model was trained and tuned on the development set using CV.\")\n",
    "print(\"The following results are computed on the separate test set:\")\n",
    "#\n",
    "predictions = selected_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "df_cm = pandas.DataFrame(cm, columns=numpy.unique(y_test), index = numpy.unique(y_test))\n",
    "sns.heatmap(df_cm, square=True, annot=True, fmt='d', cbar=False )\n",
    "pl.xlabel('predicted')\n",
    "pl.ylabel('true')\n",
    "pl.show()\n",
    "#\n",
    "print('\\n clasification report:\\n', classification_report(y_test, predictions))\n",
    "print('\\n')        \n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS: We can do a similar process to chose between different hyperparameters for a model\n",
    "\n",
    "# Train and validate all the different model versions using the (formative) data set\n",
    "hyperparameters = {'n_neighbors':[3, 4, 5, 6, 7], 'weights':['uniform','distance']}\n",
    "clf = GridSearchCV(selected_model, hyperparameters, cv=5, scoring=scoring)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Select the Most Promising Model version\n",
    "print(\"Best hyperparameters set found on development (formative) data set:\")\n",
    "print(clf.best_params_)\n",
    "print(\"Grid scores on development (formative) data set:\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "        % (mean, std * 2, params))\n",
    "print('\\n')\n",
    "\n",
    "# Report the independent final verdict using the summative data set\n",
    "print(\"Detailed classification report:\")\n",
    "print(\"The model was trained and tuned using cross-validation on the full development (formative) data set.\")\n",
    "print(\"The reported summative evaluation scores were computed on a separate test data set.\")\n",
    "print('\\n')\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
